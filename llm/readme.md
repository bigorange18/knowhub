



# 编程基础

## web框架

学习Gradio Streamlit,快速构建前端页面；

## 数据可视化分析

Pandas Matplotlib Seaborn 

## 数据库

Redis PostgreSQL MongoDB



## 协议与服务部署



RESTful API、WebSocket、gRPC,用于模型服务化

## 开发与并行编程

异步编程、多线程、多进程，提升程序性能；

## C++编程



## 经典算法

线性回归、逻辑回归、SVM、朴素贝叶斯、随机森林、K-means、PCA降维

## 现代机器学习算法

XGBoost、LightGFBM、CatBoost

## 深度学习基础

梯度反向传播、激活函数、学习CNN、LSTM

## 深度学习框架

pytorch、



### 提示工程

提示词工程(Prompt Engineering) 是有技巧的使用提示词，从而最大限度地提高LLM相应的有效性、准确性。

1. 编写清晰的说明
2. 提供参考文本
3. 将复杂的任务才分为更为简单的子任务
4. 给模型思考
5. 使用外部工具
6. 系统地测试变化

### AGT

##### embedding

语义相似的实体在空间中映射得更近，而不相似的实体映射得更远。

```

```





# 自然语言处理

## 传统NLP方法

马尔科夫链、CRF

## 预训练

Transformer、 BERT、 GPT及其变种





# 大语言模型

## 主流大语言模型

LLAMA ChatGLM Qwen、OpenAI等模型





构建网络的经典流程：

1. 定义一个可以学习参数的神经网络；
2. 遍历训练数据集
3. 计算损失；
4. 将网络参数的梯度进行反向传播；
5. 以一定规则更新网络的权重；



文本处理：

文本张量表示方法：

one-hot

word2vec

word Embedding

1. 





Liama 3、 chatGLM



### Bert模型

1. One-hot(独热编码) 
2. 词袋模型（bag of world model,BOW)
3. TF-IDF(Term Frequency-Inverse Docement)
4. N-Gram 

神经网络语言模型





word2vec





### 模型微调

是指在预训练阶段之后，使用特定任务的有标签数据对模型进行进一步的训练和调整参数。

指令微调（IT）来增强模型的额能力和可控力。IT使用（



#### P-Tuning领域模型微调







#### Instruct-Tuning指令微调



#### Loar-QLora微调



#### RLHF基于人类反馈的强化学习微调



实战代码演



## 提升工程（Prompt Engineering)





## 大模型的微调与预训练

LLam-Factory



## 大模型的量化



## 模型的部署



# 面试

### 工作要求



- [x] python
- [ ] Java
- [ ] Linux
- [ ] docker
- [ ] Mysql/PostgreSQL
- [ ] openAi
- [ ] Embedding优化
- [ ] RAG开发流程，具备检索（Retrieval）与生成（Generation）模块的开发
  
- [ ] 具备 Prompt 工程经验，能够编写高效、精准的 Prompt 模板，
- [ ] Fine-tuning
- [ ] Langchain
- [ ] Llama
- [ ] Qwen-Agent
- [ ] Hugging Face
- [ ] SFT
- [ ] RLHF

## 算法题

简单题200个

（快排、桶排）

难一点100个



## 技术面



CNN模型中池化层的作用？

Max Pooling是如何反向传递梯度的？

LSTM Transformer GPT



深度学习的三种并行方式：数据并行，模型并行，流水线并行



